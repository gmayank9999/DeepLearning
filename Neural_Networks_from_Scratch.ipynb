{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMitkzh8kXCBXCBle+rgTVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmayank9999/DeepLearning/blob/main/Neural_Networks_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RovXNmBgDVz-",
        "outputId": "0b242f84-865a-4565-ed97-5851ac7c73e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# single neuron with 3 inputs\n",
        "inputs=[1,2,3]\n",
        "weights=[0.2,0.8,-0.5]\n",
        "bias=2\n",
        "\n",
        "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 neurons\n",
        "# 4 inputs to each neuron with 3 different list of weights\n",
        "# 3 different bias\n",
        "#bias and weights are with respect to neuron\n",
        "inputs=[1,2,3,2.5]\n",
        "\n",
        "weights1=[0.2,0.8,-0.5,1.0]\n",
        "weights2=[0.5,-0.91,0.26,-0.5]\n",
        "weights3=[-0.26,-0.27,0.17,0.87]\n",
        "\n",
        "bias1=2\n",
        "bias2=3\n",
        "bias3=0.5\n",
        "\n",
        "output=[inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+bias1,\n",
        "        inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+bias2,\n",
        "        inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+bias3]\n",
        "output\n",
        "\n",
        "#by tweaking the weights and the bias, we can change the output\n",
        "# hence in deep learning our job is how best to tune the weights and biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0cq-lCHHZy",
        "outputId": "12f6c171-1ed6-492f-9d55-8271b0e594de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.8, 1.21, 2.385]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=[1,2,3,2.5]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "layer_outputs=[]\n",
        "for neuron_weights,neuron_bias in zip(weights,biases):\n",
        "    neuron_output=0\n",
        "    for n_input,weight in zip(inputs,neuron_weights):\n",
        "      neuron_output+=n_input*weight\n",
        "    neuron_output+=neuron_bias\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "print(layer_outputs )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7MZIyA1UmDA",
        "outputId": "3daa5c1c-2a9b-4afd-f7ec-e24e616cdd2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs=[1,2,3,2.5]\n",
        "weights=[0.2,0.8,-0.5,1.0]\n",
        "bias=2\n",
        "\n",
        "output=np.dot(weights,inputs)+bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR2T2_Nw0fIm",
        "outputId": "05924818-391e-4d37-c7bb-35045013f069"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs=[1,2,3,2.5]\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "output=np.dot(weights,inputs)+biases\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8n1wFlp00yY",
        "outputId": "fac379f6-0cba-48e1-c301-77c52d4e6692"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taking batches of inputs\n",
        "import numpy as np\n",
        "inputs=[[1,2,3,2.5],\n",
        "        [2,5,-1,2],\n",
        "        [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "output=np.dot(inputs,np.array(weights).T)+biases\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WOzLHpWWXpZ",
        "outputId": "da2352dd-97fd-4e4f-dea4-c486ad3f1215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with 2 layers\n",
        "import numpy as np\n",
        "inputs=[[1,2,3,2.5],\n",
        "        [2,5,-1,2],\n",
        "        [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "weights2=[[0.1,-0.14,0.5],\n",
        "        [-0.5,0.12,-0.33],\n",
        "        [-0.44,0.73,-0.13]]\n",
        "biases2=[-1,2,-0.5]\n",
        "\n",
        "layer1_output=np.dot(inputs,np.array(weights).T)+biases\n",
        "layer2_output=np.dot(layer1_output,np.array(weights2).T)+biases2\n",
        "print(layer2_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etiHVcNnXTXw",
        "outputId": "41ad2cb0-ee80-4b11-c463-7835b916e987"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# X is the input feature\n",
        "X=[[1,2,3,2.5],\n",
        "    [2,5,-1,2],\n",
        "    [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "layer1=Layer_Dense(4,5)\n",
        "layer2=Layer_Dense(5,2)\n",
        "\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_pP_4k9Yz-_",
        "outputId": "e92af5d2-3fc2-4d4b-c1d2-099243404719"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.16205863 -0.59162673 -0.13950288  0.46799897  0.9523419 ]\n",
            " [-1.02098302  0.42447985 -0.44718963  0.33623429  1.16785186]\n",
            " [ 0.34360369 -0.19537749 -0.32067728 -0.12589691  0.15467134]]\n",
            "[[-0.06160484 -0.16063992]\n",
            " [-0.15447482 -0.26389604]\n",
            " [-0.01831633  0.04863493]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHehzdWTqiz9",
        "outputId": "31d5b695-385d-41af-ac22-a31251da2e37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "X, y = spiral_data(100, 3)\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "layer1 = Layer_Dense(2,5)\n",
        "activation1 = Activation_ReLU()\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "print(\"=\"*100)\n",
        "activation1.forward(layer1.output)\n",
        "print(activation1.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8LsNPTRc63J",
        "outputId": "c6054adc-be17-4c01-d82a-9ecd5fc3fd41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00]\n",
            " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
            "   4.56846210e-05]\n",
            " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
            "   6.10024377e-04]\n",
            " ...\n",
            " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
            "  -6.71350807e-02]\n",
            " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
            "  -6.29687458e-02]\n",
            " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
            "  -6.81458861e-02]]\n",
            "====================================================================================================\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
            "  4.56846210e-05]\n",
            " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
            "  6.10024377e-04]\n",
            " ...\n",
            " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
            "  0.00000000e+00]\n",
            " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
            "  0.00000000e+00]\n",
            " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
            "  0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input--> Exponentiate --> Normalise --> Output\n",
        "#Softmax Activation =Exponentiate + Normalise\n",
        "import numpy as np\n",
        "# import math\n",
        "\n",
        "layer_outputs=[4.8,1.21,2.385]\n",
        "# E=math.e  # E is euler's number =2.7128.....\n",
        "\n",
        "exp_values=np.exp(layer_outputs)\n",
        "print(exp_values)\n",
        "\n",
        "norm_values=exp_values/np.sum(exp_values)\n",
        "print(norm_values)\n",
        "print(np.sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0sfxdYduhR6",
        "outputId": "8d8bc67d-71a5-4926-af3a-a234ad3e846d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121.51041752   3.35348465  10.85906266]\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input--> Exponentiate --> Normalise --> Output\n",
        "#Softmax Activation =Exponentiate + Normalise\n",
        "#input in batches\n",
        "layer_outputs=[[4.8,1.21,2.385],\n",
        "               [8.9,-1.81,0.2],\n",
        "               [1.41,1.051,0.026]]\n",
        "\n",
        "exp_values=np.exp(layer_outputs)\n",
        "\n",
        "# print(np.sum(exp_values,axis=1,keepdims=True))   #axis=0 gives the sum of columns while axis=1 gives the sum of rows which is what we want\n",
        "norm_values=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "print(norm_values)\n",
        "\n",
        "#Note--> e to the power a large number can lead to overflow error, hence we should subtract the maximum value from all the data points, this will cause no change to the o/p but will protect from the overflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rndonu_bwLwc",
        "outputId": "fdc5cfbd-2499-4302-bc27-e7f10781f056"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "\n",
        "X,y=spiral_data(samples=100,classes=3)\n",
        "dense1=Layer_Dense(2,3)\n",
        "activation1=Activation_ReLU()\n",
        "\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2=Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "print(activation2.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVdA7m1lxmvs",
        "outputId": "93a4e5dc-1e3a-49ed-8ed3-b93b239d3620"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33343706 0.33330157 0.3332613 ]\n",
            " [0.3334363  0.33340448 0.33315927]\n",
            " [0.33363777 0.33325708 0.33310518]\n",
            " [0.3337197  0.3332725  0.33300784]\n",
            " [0.33385548 0.33316278 0.33298174]\n",
            " [0.33386868 0.33331236 0.332819  ]\n",
            " [0.33375522 0.3335306  0.33271414]\n",
            " [0.33401015 0.33335492 0.33263496]\n",
            " [0.33405906 0.33340126 0.33253965]\n",
            " [0.3336788  0.33381182 0.33250934]\n",
            " [0.33424297 0.33338907 0.33236796]\n",
            " [0.3335921  0.33400425 0.3324037 ]\n",
            " [0.33351466 0.3340755  0.33240986]\n",
            " [0.33349293 0.33398643 0.3325206 ]\n",
            " [0.33355233 0.33423045 0.33221728]\n",
            " [0.33348194 0.33394113 0.33257693]\n",
            " [0.3335034  0.33402932 0.33246726]\n",
            " [0.33375525 0.33431962 0.3319251 ]\n",
            " [0.33379802 0.33436286 0.3318391 ]\n",
            " [0.3335962  0.3344112  0.33199257]\n",
            " [0.33343923 0.3337662  0.33279458]\n",
            " [0.33356425 0.33427945 0.3321563 ]\n",
            " [0.33360928 0.33446512 0.3319256 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.333448   0.33380195 0.3327501 ]\n",
            " [0.33371174 0.3348888  0.3313994 ]\n",
            " [0.33337337 0.3334967  0.33313   ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3335216  0.33410415 0.33237422]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33334604 0.33338526 0.33326867]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3343691  0.33279184 0.332839  ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3337354  0.3331263  0.33313823]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33587438 0.33199856 0.33212706]\n",
            " [0.3367419  0.33154085 0.33171725]\n",
            " [0.33406225 0.33295476 0.332983  ]\n",
            " [0.33849224 0.3306164  0.33089137]\n",
            " [0.33893722 0.33038026 0.33068252]\n",
            " [0.33844346 0.33064276 0.33091375]\n",
            " [0.3371405  0.3313312  0.33152828]\n",
            " [0.3393076  0.33052203 0.33017033]\n",
            " [0.33937478 0.3301747  0.33045053]\n",
            " [0.33952174 0.33070403 0.32977423]\n",
            " [0.33885768 0.330425   0.33071733]\n",
            " [0.33394757 0.33301628 0.33303615]\n",
            " [0.33702537 0.3350203  0.32795432]\n",
            " [0.33851856 0.3306048  0.33087665]\n",
            " [0.3400346  0.33070895 0.32925645]\n",
            " [0.33447555 0.33703944 0.32848504]\n",
            " [0.33949843 0.3325473  0.32795432]\n",
            " [0.33777288 0.3347471  0.32748005]\n",
            " [0.34039873 0.32961106 0.32999018]\n",
            " [0.34047332 0.3295721  0.3299546 ]\n",
            " [0.3399708  0.33234236 0.32768682]\n",
            " [0.33486602 0.33722287 0.32791108]\n",
            " [0.33890095 0.33403143 0.32706755]\n",
            " [0.33958107 0.33331132 0.32710758]\n",
            " [0.336003   0.33670792 0.32728904]\n",
            " [0.33434457 0.3375481  0.32810727]\n",
            " [0.3399446  0.33317208 0.32688332]\n",
            " [0.33887455 0.33452988 0.32659554]\n",
            " [0.33682615 0.33640623 0.32676762]\n",
            " [0.33417463 0.33682656 0.32899886]\n",
            " [0.33437976 0.33769822 0.327922  ]\n",
            " [0.33347747 0.3339228  0.3325997 ]\n",
            " [0.334015   0.33615386 0.32983106]\n",
            " [0.33432895 0.33748156 0.3281895 ]\n",
            " [0.33363757 0.33458185 0.33178058]\n",
            " [0.3340965  0.33649668 0.32940677]\n",
            " [0.33438447 0.33771837 0.32789716]\n",
            " [0.3336358  0.33457467 0.33178952]\n",
            " [0.33425862 0.33718237 0.32855904]\n",
            " [0.33397427 0.33598292 0.33004284]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3336328  0.3345621  0.3318051 ]\n",
            " [0.33394262 0.33585036 0.33020702]\n",
            " [0.3336127  0.33447918 0.3319081 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33340436 0.3336234  0.33297223]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33343646 0.3337548  0.33280867]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333388 0.33333316 0.33333293]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33425516 0.33284748 0.33289734]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3339534  0.3330074  0.33303916]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33390707 0.33303216 0.33306083]\n",
            " [0.33346722 0.33326432 0.3332684 ]\n",
            " [0.3334866  0.33325428 0.3332592 ]\n",
            " [0.33369622 0.33314377 0.33316   ]\n",
            " [0.33513978 0.33258897 0.33227128]\n",
            " [0.33484772 0.33253607 0.33261618]\n",
            " [0.33463594 0.33264816 0.33271593]\n",
            " [0.33543444 0.33222577 0.3323398 ]\n",
            " [0.33546853 0.33220825 0.33232328]\n",
            " [0.33567345 0.33235052 0.33197603]\n",
            " [0.3346673  0.33263215 0.33270055]\n",
            " [0.33588725 0.33225074 0.33186203]\n",
            " [0.33581188 0.33283237 0.3313557 ]\n",
            " [0.3355925  0.33214396 0.33226356]\n",
            " [0.3359666  0.33287778 0.33115566]\n",
            " [0.33458045 0.3344746  0.33094493]\n",
            " [0.33619747 0.33277506 0.33102748]\n",
            " [0.33530974 0.3340215  0.33066875]\n",
            " [0.3353085  0.33410674 0.33058473]\n",
            " [0.33654684 0.33260664 0.33084652]\n",
            " [0.33574158 0.33385804 0.33040044]\n",
            " [0.3369338  0.33197582 0.33109036]\n",
            " [0.33600357 0.33376586 0.33023056]\n",
            " [0.33460614 0.3350327  0.3303611 ]\n",
            " [0.33386716 0.33553493 0.3305979 ]\n",
            " [0.33436152 0.3353302  0.33030832]\n",
            " [0.3338716  0.3355535  0.33057493]\n",
            " [0.33580473 0.33439937 0.32979593]\n",
            " [0.33392468 0.33577535 0.33029994]\n",
            " [0.33392075 0.33575878 0.33032054]\n",
            " [0.33351034 0.33405787 0.3324318 ]\n",
            " [0.33393186 0.33580527 0.3302629 ]\n",
            " [0.333954   0.3358981  0.3301479 ]\n",
            " [0.33385533 0.3354856  0.33065912]\n",
            " [0.33398288 0.336019   0.3299981 ]\n",
            " [0.33373305 0.3349771  0.33128992]\n",
            " [0.33383745 0.33541116 0.3307514 ]\n",
            " [0.33404619 0.3362848  0.32966906]\n",
            " [0.3336444  0.33461007 0.33174554]\n",
            " [0.33366978 0.33471516 0.33161503]\n",
            " [0.3339861  0.3360326  0.3299813 ]\n",
            " [0.33343786 0.33376038 0.3328018 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33363596 0.3345753  0.3317888 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33621338 0.33182213 0.3319645 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3340551  0.3329608  0.33298406]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33370972 0.3331432  0.33314702]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.34065023 0.3294812  0.32986853]\n",
            " [0.33991662 0.32986915 0.33021423]\n",
            " [0.33817664 0.33078763 0.3310357 ]\n",
            " [0.3377908  0.33099133 0.33121789]\n",
            " [0.34148645 0.32903975 0.32947376]\n",
            " [0.34114766 0.32921955 0.32963276]\n",
            " [0.34165376 0.32895193 0.3293943 ]\n",
            " [0.34129685 0.32914126 0.32956192]\n",
            " [0.33696926 0.3314253  0.33160543]\n",
            " [0.34195888 0.32879147 0.3292497 ]\n",
            " [0.34176135 0.3288964  0.32934222]\n",
            " [0.342371   0.33034304 0.327286  ]\n",
            " [0.34161022 0.33224156 0.32614824]\n",
            " [0.34260407 0.3284507  0.32894525]\n",
            " [0.34208617 0.3317146  0.32619923]\n",
            " [0.33733404 0.33698034 0.32568568]\n",
            " [0.34306756 0.32918817 0.32774428]\n",
            " [0.33580333 0.3381637  0.3260329 ]\n",
            " [0.34318176 0.32981095 0.3270073 ]\n",
            " [0.34109834 0.33379012 0.3251115 ]\n",
            " [0.3406086  0.33444968 0.32494178]\n",
            " [0.33983096 0.3353351  0.32483393]\n",
            " [0.34212467 0.33278525 0.3250901 ]\n",
            " [0.34074238 0.33458722 0.32467043]\n",
            " [0.3403309  0.33510435 0.32456473]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.333348   0.33339322 0.33325875]\n",
            " [0.33334798 0.33339304 0.33325905]\n",
            " [0.33336633 0.33346808 0.3331656 ]\n",
            " [0.33343583 0.33354747 0.33301675]\n",
            " [0.33340082 0.33360893 0.33299023]\n",
            " [0.33339214 0.33357346 0.33303437]\n",
            " [0.33342543 0.3337096  0.332865  ]\n",
            " [0.33343577 0.33375204 0.33281216]\n",
            " [0.3334368  0.33375612 0.3328071 ]\n",
            " [0.3333895  0.33356258 0.3330479 ]\n",
            " [0.3334803  0.33393455 0.3325851 ]\n",
            " [0.33345786 0.33384246 0.33269963]\n",
            " [0.33338755 0.33355466 0.3330578 ]\n",
            " [0.3333875  0.33355433 0.3330582 ]\n",
            " [0.33337742 0.33351326 0.33310932]\n",
            " [0.33342582 0.3337112  0.33286303]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3335406  0.33418217 0.3322772 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3335174  0.33324018 0.33324242]\n",
            " [0.3357621  0.3320559  0.33218205]\n",
            " [0.33455342 0.3326939  0.33275267]\n",
            " [0.33468056 0.33262694 0.3326925 ]\n",
            " [0.33523023 0.332337   0.33243275]\n",
            " [0.3358508  0.3320096  0.33213955]\n",
            " [0.33390686 0.3330354  0.3330577 ]\n",
            " [0.33608517 0.33188617 0.33202863]\n",
            " [0.33707276 0.33136475 0.3315625 ]\n",
            " [0.33451033 0.3327175  0.33277223]\n",
            " [0.33797008 0.3308898  0.33114007]\n",
            " [0.33813232 0.3311288  0.33073887]\n",
            " [0.3372474  0.33127326 0.33147934]\n",
            " [0.33802205 0.3308638  0.33111414]\n",
            " [0.33764288 0.3310646  0.33129245]\n",
            " [0.3384622  0.33166578 0.3298721 ]\n",
            " [0.33810508 0.33082074 0.3310742 ]\n",
            " [0.3387643  0.3306828  0.33055288]\n",
            " [0.33878177 0.3304627  0.33075556]\n",
            " [0.3387768  0.33184904 0.3293742 ]\n",
            " [0.3385776  0.3305718  0.3308506 ]\n",
            " [0.3359064  0.33547533 0.3286183 ]\n",
            " [0.339297   0.3309203  0.32978272]\n",
            " [0.33804414 0.33361578 0.32834005]\n",
            " [0.33826232 0.33345228 0.32828534]\n",
            " [0.33893314 0.3326132  0.32845366]\n",
            " [0.3389117  0.3327872  0.32830107]\n",
            " [0.33421013 0.3369767  0.3288132 ]\n",
            " [0.33745742 0.33470708 0.32783547]\n",
            " [0.33449334 0.33696157 0.32854512]\n",
            " [0.33723873 0.33507404 0.32768726]\n",
            " [0.33622664 0.33597234 0.327801  ]\n",
            " [0.33422583 0.33704334 0.32873082]\n",
            " [0.3340842  0.33644503 0.32947072]\n",
            " [0.3342861  0.33729914 0.3284148 ]\n",
            " [0.3362919  0.33621648 0.32749164]\n",
            " [0.33338475 0.33354327 0.33307195]\n",
            " [0.33433518 0.3375081  0.32815674]\n",
            " [0.33463582 0.33749157 0.32787263]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33413425 0.33665594 0.3292098 ]\n",
            " [0.3343644  0.33763272 0.3280028 ]\n",
            " [0.33410946 0.33655125 0.3293393 ]\n",
            " [0.33351088 0.33406    0.33242908]\n",
            " [0.33349925 0.3340122  0.3324885 ]\n",
            " [0.3339475  0.33587086 0.33018163]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33356708 0.33429125 0.33214164]\n",
            " [0.3338295  0.335378   0.33079252]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.333501   0.33401936 0.33247963]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33365664 0.3346607  0.33168268]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3395368  0.33007288 0.3303904 ]\n",
            " [0.3405764  0.3295246  0.32989898]\n",
            " [0.3366191  0.33161187 0.33176908]\n",
            " [0.33686    0.33148488 0.33165509]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJYxQtNoxfFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}