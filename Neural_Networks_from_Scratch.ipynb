{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDISfNoC3RO3PqIxGON6VM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmayank9999/DeepLearning/blob/main/Neural_Networks_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RovXNmBgDVz-",
        "outputId": "0b242f84-865a-4565-ed97-5851ac7c73e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# single neuron with 3 inputs\n",
        "inputs=[1,2,3]\n",
        "weights=[0.2,0.8,-0.5]\n",
        "bias=2\n",
        "\n",
        "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 neurons\n",
        "# 4 inputs to each neuron with 3 different list of weights\n",
        "# 3 different bias\n",
        "#bias and weights are with respect to neuron\n",
        "inputs=[1,2,3,2.5]\n",
        "\n",
        "weights1=[0.2,0.8,-0.5,1.0]\n",
        "weights2=[0.5,-0.91,0.26,-0.5]\n",
        "weights3=[-0.26,-0.27,0.17,0.87]\n",
        "\n",
        "bias1=2\n",
        "bias2=3\n",
        "bias3=0.5\n",
        "\n",
        "output=[inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+bias1,\n",
        "        inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+bias2,\n",
        "        inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+bias3]\n",
        "output\n",
        "\n",
        "#by tweaking the weights and the bias, we can change the output\n",
        "# hence in deep learning our job is how best to tune the weights and biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0cq-lCHHZy",
        "outputId": "12f6c171-1ed6-492f-9d55-8271b0e594de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.8, 1.21, 2.385]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=[1,2,3,2.5]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "layer_outputs=[]\n",
        "for neuron_weights,neuron_bias in zip(weights,biases):\n",
        "    neuron_output=0\n",
        "    for n_input,weight in zip(inputs,neuron_weights):\n",
        "      neuron_output+=n_input*weight\n",
        "    neuron_output+=neuron_bias\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "print(layer_outputs )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7MZIyA1UmDA",
        "outputId": "3daa5c1c-2a9b-4afd-f7ec-e24e616cdd2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs=[1,2,3,2.5]\n",
        "weights=[0.2,0.8,-0.5,1.0]\n",
        "bias=2\n",
        "\n",
        "output=np.dot(weights,inputs)+bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR2T2_Nw0fIm",
        "outputId": "05924818-391e-4d37-c7bb-35045013f069"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs=[1,2,3,2.5]\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "output=np.dot(weights,inputs)+biases\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8n1wFlp00yY",
        "outputId": "fac379f6-0cba-48e1-c301-77c52d4e6692"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taking batches of inputs\n",
        "import numpy as np\n",
        "inputs=[[1,2,3,2.5],\n",
        "        [2,5,-1,2],\n",
        "        [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "output=np.dot(inputs,np.array(weights).T)+biases\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WOzLHpWWXpZ",
        "outputId": "da2352dd-97fd-4e4f-dea4-c486ad3f1215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with 2 layers\n",
        "import numpy as np\n",
        "inputs=[[1,2,3,2.5],\n",
        "        [2,5,-1,2],\n",
        "        [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "weights=[[0.2,0.8,-0.5,1.0],\n",
        "        [0.5,-0.91,0.26,-0.5],\n",
        "        [-0.26,-0.27,0.17,0.87]]\n",
        "biases=[2,3,0.5]\n",
        "\n",
        "weights2=[[0.1,-0.14,0.5],\n",
        "        [-0.5,0.12,-0.33],\n",
        "        [-0.44,0.73,-0.13]]\n",
        "biases2=[-1,2,-0.5]\n",
        "\n",
        "layer1_output=np.dot(inputs,np.array(weights).T)+biases\n",
        "layer2_output=np.dot(layer1_output,np.array(weights2).T)+biases2\n",
        "print(layer2_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etiHVcNnXTXw",
        "outputId": "41ad2cb0-ee80-4b11-c463-7835b916e987"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# X is the input feature\n",
        "X=[[1,2,3,2.5],\n",
        "    [2,5,-1,2],\n",
        "    [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "layer1=Layer_Dense(4,5)\n",
        "layer2=Layer_Dense(5,2)\n",
        "\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_pP_4k9Yz-_",
        "outputId": "e92af5d2-3fc2-4d4b-c1d2-099243404719"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.16205863 -0.59162673 -0.13950288  0.46799897  0.9523419 ]\n",
            " [-1.02098302  0.42447985 -0.44718963  0.33623429  1.16785186]\n",
            " [ 0.34360369 -0.19537749 -0.32067728 -0.12589691  0.15467134]]\n",
            "[[-0.06160484 -0.16063992]\n",
            " [-0.15447482 -0.26389604]\n",
            " [-0.01831633  0.04863493]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHehzdWTqiz9",
        "outputId": "31d5b695-385d-41af-ac22-a31251da2e37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "X, y = spiral_data(100, 3)\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "layer1 = Layer_Dense(2,5)\n",
        "activation1 = Activation_ReLU()\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "print(\"=\"*100)\n",
        "activation1.forward(layer1.output)\n",
        "print(activation1.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8LsNPTRc63J",
        "outputId": "c6054adc-be17-4c01-d82a-9ecd5fc3fd41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00]\n",
            " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
            "   4.56846210e-05]\n",
            " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
            "   6.10024377e-04]\n",
            " ...\n",
            " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
            "  -6.71350807e-02]\n",
            " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
            "  -6.29687458e-02]\n",
            " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
            "  -6.81458861e-02]]\n",
            "====================================================================================================\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
            "  4.56846210e-05]\n",
            " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
            "  6.10024377e-04]\n",
            " ...\n",
            " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
            "  0.00000000e+00]\n",
            " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
            "  0.00000000e+00]\n",
            " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
            "  0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input--> Exponentiate --> Normalise --> Output\n",
        "#Softmax Activation =Exponentiate + Normalise\n",
        "import numpy as np\n",
        "# import math\n",
        "\n",
        "layer_outputs=[4.8,1.21,2.385]\n",
        "# E=math.e  # E is euler's number =2.7128.....\n",
        "\n",
        "exp_values=np.exp(layer_outputs)\n",
        "print(exp_values)\n",
        "\n",
        "norm_values=exp_values/np.sum(exp_values)\n",
        "print(norm_values)\n",
        "print(np.sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0sfxdYduhR6",
        "outputId": "8d8bc67d-71a5-4926-af3a-a234ad3e846d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121.51041752   3.35348465  10.85906266]\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input--> Exponentiate --> Normalise --> Output\n",
        "#Softmax Activation =Exponentiate + Normalise\n",
        "#input in batches\n",
        "layer_outputs=[[4.8,1.21,2.385],\n",
        "               [8.9,-1.81,0.2],\n",
        "               [1.41,1.051,0.026]]\n",
        "\n",
        "exp_values=np.exp(layer_outputs)\n",
        "\n",
        "# print(np.sum(exp_values,axis=1,keepdims=True))   #axis=0 gives the sum of columns while axis=1 gives the sum of rows which is what we want\n",
        "norm_values=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "print(norm_values)\n",
        "\n",
        "#Note--> e to the power a large number can lead to overflow error, hence we should subtract the maximum value from all the data points, this will cause no change to the o/p but will protect from the overflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rndonu_bwLwc",
        "outputId": "fdc5cfbd-2499-4302-bc27-e7f10781f056"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "     self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "     self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "      self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "\n",
        "X,y=spiral_data(samples=100,classes=3)\n",
        "dense1=Layer_Dense(2,3)\n",
        "activation1=Activation_ReLU()\n",
        "\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2=Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "print(activation2.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVdA7m1lxmvs",
        "outputId": "c3636af5-cf77-4123-99a8-12e9a0db44eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33337906 0.33331284 0.33330804]\n",
            " [0.33339453 0.3333061  0.3332993 ]\n",
            " [0.3335186  0.33324987 0.33323154]\n",
            " [0.3334388  0.33328655 0.3332746 ]\n",
            " [0.3335358  0.33324292 0.33322126]\n",
            " [0.33360627 0.33321124 0.33318245]\n",
            " [0.3337713  0.33313632 0.3330924 ]\n",
            " [0.33378714 0.3331297  0.33308312]\n",
            " [0.33378202 0.3331324  0.33308554]\n",
            " [0.33368942 0.33317465 0.33313596]\n",
            " [0.33399874 0.33303437 0.3329669 ]\n",
            " [0.33401024 0.33302087 0.33296886]\n",
            " [0.33408633 0.33299536 0.33291835]\n",
            " [0.3341989  0.33294323 0.3328579 ]\n",
            " [0.33426663 0.33291283 0.33282056]\n",
            " [0.3341416  0.33294335 0.33291498]\n",
            " [0.33438173 0.33286196 0.33275628]\n",
            " [0.33334744 0.33332545 0.33332714]\n",
            " [0.3344966  0.33281046 0.33269295]\n",
            " [0.33451894 0.33279783 0.33268327]\n",
            " [0.33380824 0.333068   0.33312374]\n",
            " [0.33406496 0.33292454 0.33301046]\n",
            " [0.3336738  0.3331431  0.33318308]\n",
            " [0.33439115 0.3327962  0.33281267]\n",
            " [0.33402696 0.33294576 0.33302724]\n",
            " [0.33386466 0.33303645 0.3330989 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.333837   0.33305192 0.33311108]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33323887 0.33333042 0.3334307 ]\n",
            " [0.33298343 0.3333313  0.33368528]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33311453 0.33332664 0.33355886]\n",
            " [0.33313233 0.33332714 0.33354053]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3329527  0.33332157 0.33372572]\n",
            " [0.33294946 0.33334294 0.33370754]\n",
            " [0.33297727 0.33337706 0.3336457 ]\n",
            " [0.3329278  0.3333344  0.33373776]\n",
            " [0.33297548 0.33338767 0.33363682]\n",
            " [0.33306807 0.33332515 0.33360675]\n",
            " [0.3329563  0.33338434 0.33365938]\n",
            " [0.33379313 0.33313775 0.3330691 ]\n",
            " [0.33479592 0.33268562 0.3325185 ]\n",
            " [0.33302724 0.3334467  0.33352605]\n",
            " [0.33411837 0.33299175 0.33288988]\n",
            " [0.33475873 0.332703   0.33253822]\n",
            " [0.33511034 0.3325444  0.33234525]\n",
            " [0.3360721  0.33210933 0.33181858]\n",
            " [0.33293036 0.33341053 0.33365917]\n",
            " [0.3358441  0.33221313 0.3319428 ]\n",
            " [0.33665884 0.33184382 0.33149728]\n",
            " [0.334859   0.33265895 0.332482  ]\n",
            " [0.33769292 0.33136615 0.33094093]\n",
            " [0.33744538 0.33148652 0.3310681 ]\n",
            " [0.33757722 0.33139622 0.33102658]\n",
            " [0.33687115 0.33174887 0.33138   ]\n",
            " [0.3370468  0.33166927 0.33128387]\n",
            " [0.33811852 0.33117712 0.33070433]\n",
            " [0.33653462 0.3319024  0.33156297]\n",
            " [0.3381473  0.33116117 0.33069146]\n",
            " [0.3381633  0.33115333 0.33068335]\n",
            " [0.3362847  0.33173183 0.33198348]\n",
            " [0.33658463 0.33162355 0.3317918 ]\n",
            " [0.33828554 0.33109736 0.33061707]\n",
            " [0.33855486 0.3309795  0.3304656 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33816382 0.33110607 0.3307301 ]\n",
            " [0.3383073  0.33105692 0.33063573]\n",
            " [0.3380773  0.33110854 0.33081412]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3359804  0.33185503 0.33216453]\n",
            " [0.33608028 0.3317993  0.33212045]\n",
            " [0.33347556 0.33325386 0.33327058]\n",
            " [0.33384416 0.33304793 0.33310792]\n",
            " [0.33454174 0.33265826 0.33280003]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33430877 0.33278835 0.33290288]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33323908 0.33333343 0.33342746]\n",
            " [0.3337548  0.333146   0.33309925]\n",
            " [0.33325577 0.33333096 0.33341324]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33325094 0.3333653  0.33338374]\n",
            " [0.33322573 0.33333004 0.3334442 ]\n",
            " [0.33324075 0.33333048 0.33342874]\n",
            " [0.33321133 0.33335516 0.33343348]\n",
            " [0.33325762 0.33333102 0.33341134]\n",
            " [0.3332245  0.3333735  0.33340195]\n",
            " [0.33321214 0.33337104 0.3334168 ]\n",
            " [0.3332943  0.33335552 0.3333502 ]\n",
            " [0.3336705  0.33318627 0.33314323]\n",
            " [0.33359432 0.33322075 0.3331849 ]\n",
            " [0.33372965 0.33315992 0.3331104 ]\n",
            " [0.3341311  0.33297896 0.33288988]\n",
            " [0.33441734 0.3328498  0.33273286]\n",
            " [0.33336195 0.3333262  0.33331186]\n",
            " [0.33319622 0.33339834 0.33340546]\n",
            " [0.3346313  0.33275366 0.33261508]\n",
            " [0.3335763  0.33323023 0.33319345]\n",
            " [0.33510578 0.33253887 0.33235532]\n",
            " [0.33552817 0.33234543 0.33212644]\n",
            " [0.3355766  0.33232403 0.33209935]\n",
            " [0.33485514 0.3326535  0.33249134]\n",
            " [0.33521664 0.33248976 0.33229357]\n",
            " [0.33578333 0.33222976 0.33198693]\n",
            " [0.3357811  0.3322327  0.33198613]\n",
            " [0.33590087 0.33217773 0.33192143]\n",
            " [0.33559722 0.33226785 0.33213493]\n",
            " [0.33597115 0.3321472  0.3318817 ]\n",
            " [0.33605793 0.33210483 0.33183727]\n",
            " [0.33615458 0.33206207 0.33178338]\n",
            " [0.33622128 0.33203214 0.33174658]\n",
            " [0.33619747 0.33204114 0.33176136]\n",
            " [0.33434343 0.332769   0.33288756]\n",
            " [0.33487758 0.33247072 0.3326517 ]\n",
            " [0.33587044 0.3321106  0.33201894]\n",
            " [0.33479428 0.3325172  0.33268848]\n",
            " [0.3363644  0.3319523  0.33168328]\n",
            " [0.33497545 0.33241606 0.33260852]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3335225  0.3332276  0.33324987]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33354688 0.333214   0.3332391 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33305967 0.3333249  0.33361542]\n",
            " [0.33324367 0.3333306  0.3334257 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3331263  0.3334407  0.33343303]\n",
            " [0.33289602 0.3334294  0.33367458]\n",
            " [0.33327532 0.33333156 0.33339313]\n",
            " [0.33285654 0.33331853 0.33382493]\n",
            " [0.33291286 0.33345544 0.33363172]\n",
            " [0.33292434 0.33332068 0.33375502]\n",
            " [0.33286074 0.3334315  0.33370772]\n",
            " [0.33308727 0.33345953 0.33345312]\n",
            " [0.33304816 0.33332458 0.33362728]\n",
            " [0.33272257 0.33332175 0.3339557 ]\n",
            " [0.33418918 0.33296394 0.33284694]\n",
            " [0.33351314 0.33326864 0.33321822]\n",
            " [0.33526826 0.3324774  0.33225435]\n",
            " [0.3330009  0.33349958 0.33349952]\n",
            " [0.33312273 0.33344495 0.33343232]\n",
            " [0.33275333 0.33331525 0.33393136]\n",
            " [0.3328365  0.33346695 0.3336966 ]\n",
            " [0.33573276 0.33226854 0.33199868]\n",
            " [0.33795068 0.33126342 0.33078593]\n",
            " [0.3378342  0.33131668 0.33084908]\n",
            " [0.33690384 0.33173922 0.33135688]\n",
            " [0.3370801  0.3316596  0.33126035]\n",
            " [0.33849475 0.3310169  0.33048832]\n",
            " [0.3386468  0.33094785 0.3304053 ]\n",
            " [0.33932027 0.33063024 0.33004943]\n",
            " [0.33932897 0.3306361  0.3300349 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3333729  0.33331567 0.33331144]\n",
            " [0.33342808 0.33328652 0.3332854 ]\n",
            " [0.33348477 0.3332655  0.33324972]\n",
            " [0.33355498 0.33323026 0.33321473]\n",
            " [0.33364704 0.3331921  0.33316082]\n",
            " [0.3337046  0.33316603 0.33312938]\n",
            " [0.33375472 0.33314326 0.33310202]\n",
            " [0.3335228  0.33322746 0.33324975]\n",
            " [0.33382222 0.33310413 0.33307365]\n",
            " [0.33379087 0.33310425 0.3331049 ]\n",
            " [0.3340238  0.3330225  0.33295366]\n",
            " [0.33367893 0.33314022 0.33318081]\n",
            " [0.33353215 0.33322224 0.3332456 ]\n",
            " [0.33358118 0.33319485 0.33322397]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3335214  0.33322826 0.33325034]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3340451  0.33294943 0.3330055 ]\n",
            " [0.3334036  0.33329403 0.33330232]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33308125 0.333326   0.3335927 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3333215  0.33333296 0.33334547]\n",
            " [0.3333135  0.33333275 0.33335376]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3330544  0.33333835 0.33360723]\n",
            " [0.33315906 0.33341852 0.33342242]\n",
            " [0.33314598 0.33332756 0.3335264 ]\n",
            " [0.33316675 0.33332822 0.33350503]\n",
            " [0.33311734 0.33332667 0.33355597]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33301464 0.33334276 0.33364263]\n",
            " [0.333072   0.3333969  0.3335311 ]\n",
            " [0.33394578 0.33306643 0.3329878 ]\n",
            " [0.33411607 0.33298984 0.33289406]\n",
            " [0.3330469  0.33339566 0.33355743]\n",
            " [0.33308986 0.33342665 0.3334835 ]\n",
            " [0.3352668  0.3324705  0.3322627 ]\n",
            " [0.3336593  0.33319652 0.33314416]\n",
            " [0.33344236 0.3332944  0.33326325]\n",
            " [0.33633614 0.33198565 0.33167818]\n",
            " [0.33625934 0.33202106 0.33171958]\n",
            " [0.33532077 0.33244723 0.33223197]\n",
            " [0.33471075 0.33272326 0.332566  ]\n",
            " [0.3330302  0.3334299  0.33353993]\n",
            " [0.33677688 0.3317864  0.33143672]\n",
            " [0.3352932  0.33246064 0.3322462 ]\n",
            " [0.33377078 0.33314803 0.3330812 ]\n",
            " [0.33681425 0.33172122 0.33146453]\n",
            " [0.33729243 0.3315491  0.33115846]\n",
            " [0.3363014  0.3320048  0.33169386]\n",
            " [0.33651683 0.33190715 0.33157596]\n",
            " [0.33690888 0.331653   0.33143815]\n",
            " [0.3373513  0.33152705 0.33112162]\n",
            " [0.3355558  0.33209205 0.33235216]\n",
            " [0.33649194 0.33175087 0.33175713]\n",
            " [0.33630934 0.33180127 0.33188948]\n",
            " [0.33621377 0.33182466 0.3319616 ]\n",
            " [0.33627707 0.33179587 0.33192703]\n",
            " [0.3346379  0.33260453 0.33275753]\n",
            " [0.33683634 0.33159983 0.3315638 ]\n",
            " [0.3354232  0.33216608 0.33241078]\n",
            " [0.33369702 0.3331301  0.33317286]\n",
            " [0.3336737  0.33314314 0.33318314]\n",
            " [0.33689132 0.3315483  0.33156043]\n",
            " [0.33509484 0.33234942 0.33255577]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33405876 0.33292803 0.3330132 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33311883 0.33332673 0.33355445]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3332338  0.33333027 0.33343592]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical Cross Entropy for Loss Function\n",
        "\n",
        "import math\n",
        "\n",
        "softmax_output=[0.7,0.1,0.2]\n",
        "target_output=[1,0,0]\n",
        "loss=-(math.log(softmax_output[0])*target_output[0]+\n",
        "       math.log(softmax_output[1])*target_output[1]+\n",
        "       math.log(softmax_output[2])*target_output[2])\n",
        "print(loss)\n",
        "loss=-math.log(softmax_output[0])\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "204Bjaym4Y7G",
        "outputId": "8023751d-cafa-4169-f80a-741112d3b006"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n",
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSJ4PoZ_5SSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}